---
title: "p8105_hw3_eh2928"
author: "Emil Hafeez (eh2928)"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

#Basic Setup
```{r setup, results = 'hide'}
knitr::opts_chunk$set(echo = TRUE)

#Load relevant libraries, including for Problems 1 and 3
library(tidyverse)
library(p8105.datasets)
library(patchwork)

#Prep for neater output
knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)

#Trying a darker theme, at least at first
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```
#Problem 1

```{r load in, explore}
data("instacart")
```
This large dataset describes online grocery store purchases and contains `r ncol(instacart)` columns and a whopping `r nrow(instacart)` rows, where each row is a product from a user's order. The products are organized by characteristics like the department, aisle, name, and numeric code, plus whether the product was reordered by the user, in what order it was added to the user's cart, the day of the week, days since its last order, and more. The users are demarcated by ID. 
There are `r instacart %>% distinct(department) %>% count()` distinct departments and `r instacart %>% distinct(aisle) %>% count()` distinct aisles.

```{r answer to how many aisles and which are most popular, results = 'hide'}
instacart %>% 
  count(aisle) %>% 
    arrange(desc(n))
```
Fresh vegetables, fresh fruits, and packaged vegetables & fruits are the top three aisles from which products are most commonly ordered. 
```{r plot per-aisle orders, limiting to 10,000+ orders}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
  labs(
    title = "Instacart 2017 Orders over 10,000 units per Aisle",
    x = "Aisle Name",
    y = "Number of Product Orders per Aisle") +
  scale_y_continuous(breaks = seq(0,151000,20000)) + 
	theme(axis.text.x = element_text(angle = -90, vjust = .5, hjust = 1)) +
  theme(axis.title.y = element_text(angle = 90, vjust = .5, hjust = 1))

```
It appears that fresh vegetables and fresh fruits in particular are ordered from Instacart with a much higher frequency than other aisles, where packaged fruits and vegetables is a distant yet still popular third. There is a sharp decrease to less popular aisles after the first two, and the frequency levels out steadily after the first ten or so most popular aisles.

Table time! Most popular items in each of 3 specific aisles, along with the number of times they were ordered.
```{r table}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
  mutate(product_name = tolower(product_name)) %>% 
	knitr::kable(
              col.names =
                c("Aisle", "Product Name", "Orders", "Rank within Aisle"),
              align = "cccc")
```
Packaged fruits have the most orders among these (as noted, a very popular category). Spinach, raspberries, and blueberries are the most popular. Baking products (brown sugar, baking soda, cane sugar) sound great. Dog food and care have low order numbers in comparison, and are in order of treats, food, and biscuits.
```{r table time part two}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(
    order_dow = recode(order_dow, `0` = "Sunday", `1` = "Monday", `2` = "Tuesday", `3` = "Wednesday", 
                   `4` = "Thursday", `5` = "Friday", `6` = "Saturday") 
        ) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```
Here, we can see that the mean hour of product ordering tends to be between the 11.0 and 15.5 hours of the day (11am and 3:30pm). The overall latest ordering day of the week seems to be Wednesday, and the earliest appears to be Friday. 

# Problem Two
```{r}
accel_df =
  read_csv(
      "./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_per_day",
    values_to = "activity_count"
  ) %>% 
    mutate(minute_per_day = str_remove_all(minute_per_day, "activity_"),
    minute_per_day = as.numeric(minute_per_day)) %>% 
    rename(day_of_week = day) %>% 
    mutate(day_of_week = as.factor(day_of_week),
         day_of_week = forcats::fct_relevel(day_of_week, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")))  %>% 
    arrange(week,day_of_week) %>% 
    mutate(
      weekend = case_when(
        day_of_week %in% c("Monday","Tuesday","Wednesday","Thursday","Friday") ~ "weekday",
        day_of_week %in% c("Saturday","Sunday") ~ "weekend"),
      weekend = as.factor(weekend)
      )
```
This dataset has `r ncol(accel_df)` columns now that it's been pivoted wide to long. It's tidied, using rows rather than columns to represent each minute of each of the 35 days, for a total of (35*1440) = `n nrow(accel_df)` rows. Overall, it uses a week, then day of the week, then minute per day structure to characterize activity counts over time.

Now, we aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. 
```{r use group and kable to create this variable}
accel_df %>% 
  group_by(weekend, week, day_of_week, day_id) %>% #Choosing to include weekend here, so as to investigate trends more clearly
  summarize(total_activity_daily = sum(activity_count)) %>% 
  arrange(
    desc(total_activity_daily)) %>% 
  knitr::kable(
              col.names =
                c("Weekend", "Week", "Day of Week", "Day Identifier", "Total Daily Activity"),
              align = "ccccc", 
              digits = 1)
```
There are no strongly identifiable trends in terms of which individual days display highest total daily activity, though it does appear that the weekend and weekdays adjacent to the weekend have more activity than days in the middle of the week.
```{r}
ggp_24_hour_activities =
accel_df %>% 
  ggplot(aes(x = minute_per_day, y = activity_count)) + 
  geom_line(aes(color = day_of_week), alpha = .15) + 
  geom_smooth(aes(color = day_of_week), alpha = 0.2, size = .75) +
   scale_x_continuous(
    breaks = c(0, 360, 720, 1080, 1440), 
    labels = c("12am", "6am", "12pm", "6pm", "12am")) +
  labs(
    title = "Daily Activity Plot",
    x = "Minute Per Day (1440 per 24 hours)",
    y = "Activity Count (per accelerometer)",
    caption = "63 year old male, BMI 25, Congestive Heart Failure, Columbia University Medical Center") 
ggp_24_hour_activities
```
This single-panel plot showing the 24 hour time courses for the individual's activity for each day (parsed by day of the week) helps illustrates that the individual tends to be less active between the hours of 12am and about 9am every day of the week (presumably asleep), as well as between about 3pm and 6pm. Overall, the individual appears to show more activity during the late morning, and in the evening between about 7:30pm and 10:30pm. Regarding days of the week, it appears that Sundays tend to be more active than other days in the late morning, and that Fridays are the most active of all days in the late evening. (Quick note: I'm (very) deuteronopic, and even with the veridis package and trying different themes, sizes, and so on, still a little hard to tell. I'll reach out for advice.)

# Problem 3
```{r load}
data("ny_noaa")
```
```{r setup and EDA}
ny_noaa =
ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day_of_month")) %>% 
  mutate(
        tmax = as.numeric(tmax),
        tmin = as.numeric(tmin),
        year = as.numeric(year),
        month = as.numeric(month),
        day_of_month = as.numeric(day_of_month)
        ) %>% 
  mutate(
        prcp = (prcp / 10),
        tmax = (tmax / 10),
        tmin = (tmin / 10),
        )
ny_noaa %>% 
  count(snow) %>% 
	arrange(desc(n))
```
The NY NOAA dataset is a large rectangle of `r nrow(ny_noaa)` weather observations in NY state, structured by `r distinct(ny_noaa, id) %>% count()` weather stations at which the data were collected, in conjunction with the date of that observation. Then, there is the total precipitation (in tenths of millimeters), specifically snow characteristics, and the temperature minimum and maximum (in celsius). 

The most commonly observed snowfall value by far is 0 (since most days in these locations, it doesn't snow!). Then, unfortunately, there are a lot of missing values for snowfall (potentially measurement error, potentially a null value), and then various millimeter values. 

### Problem 3 Part 2

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
ggp_max_temps =
ny_noaa %>% 
  filter(month %in% c(1 , 7)) %>% 
  group_by(id, year, month) %>%
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = mean_tmax, group = id)) + 
  geom_point(aes(color = mean_tmax), alpha = .5) + 
  facet_grid(. ~ month) +
  labs(
    title = "Mean Maximum Temperatures in NY State, January and July",
    x = "Year",
    y = "Mean Max Temperature (in °C)",
    caption = "From NY_NOAA Dataset")
```
The graph shows a distinctly higher average max temperature in July versus January, and a few outliers in both months. There appear to be more max temperature irregularities in January than in July; additionally, January shows a few uncharacteristically warm days and July shows a few uncharacteristically colder days. Overall, of concern and not unrelated to narratives regarding global warming trends. 

Finally, we make a two-panel plot showing tmax vs tmin for the full dataset, and then make a plot showing the distribution of snowfall values greater than 0 and less than 100.

```{r hexplot}
ggp_max_min_hex = 
ny_noaa %>% 
  select(id, year, month, tmax, tmin) %>%
  pivot_longer(
      tmax:tmin,
      names_to = "observation",
      values_to = "temperature"
              ) %>% 
  ggplot(aes(x = year, y = temperature)) + 
  geom_hex(aes(color = temperature), alpha = .5) + 
  facet_grid(. ~ observation) +
  labs(
    title = "Distributions of Maximum and Minimum Temperatures in NY State, 1981-2010",
    x = "Year",
    y = "Mean Max Temperature (in °C)",
    caption = "From NY_NOAA Dataset") +
  theme(plot.title = element_text(hjust = .5))
```
```{r snowfall dists}
ggp_snow = 
ny_noaa %>% 
  select(id, year, month, tmax, tmin, snow) %>%
  filter(snow > 0, snow < 100) %>% 
  ggplot(aes(x = year, y = snow, color = year, group = year)) + 
  geom_boxplot(alpha = .4) + 
  labs(
    title = "Distributions of Snowfall in NY State, 1981-2010",
    x = "Year",
    y = "Snowfall in millimeters",
    caption = "From NY_NOAA Dataset") +
  theme(plot.title = element_text(hjust = .5))

ggp_max_min_hex / ggp_snow
```

